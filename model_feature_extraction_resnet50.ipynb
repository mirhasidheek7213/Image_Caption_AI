{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWaEFyPjUzWb",
        "outputId": "2255531d-f90a-4a3b-d8bd-072f604c3049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Set file paths\n",
        "IMAGE_FOLDER = \"/content/drive/MyDrive/data/loadedimages\"\n",
        "CAPTIONS_FILE = \"/content/drive/MyDrive/data/filtered_captions.tsv\"\n",
        "OUTPUT_FILE = \"/content/drive/MyDrive/data/filtered_captions_cleaned.tsv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzTleGowaZl6",
        "outputId": "2e905ea4-d2f9-4114-8cfc-10585ef89cf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Filtered captions: 53367\n",
            " Saved to: /content/drive/MyDrive/data/filtered_captions_cleaned.tsv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Load captions file\n",
        "df = pd.read_csv(CAPTIONS_FILE, sep=\"\\t\", names=[\"image\", \"caption\"])\n",
        "df = df.dropna()\n",
        "df[\"image\"] = df[\"image\"].astype(str).str.strip()\n",
        "\n",
        "# Ensure .jpg extension\n",
        "df[\"image\"] = df[\"image\"].apply(lambda x: x if x.endswith(\".jpg\") else x + \".jpg\")\n",
        "\n",
        "# List of images in the folder\n",
        "image_files = set([f.strip() for f in os.listdir(IMAGE_FOLDER)])\n",
        "\n",
        "# Filter rows where image exists\n",
        "df_filtered = df[df[\"image\"].isin(image_files)].reset_index(drop=True)\n",
        "\n",
        "# Save cleaned captions\n",
        "OUTPUT_FILE = \"/content/drive/MyDrive/data/filtered_captions_cleaned.tsv\"\n",
        "df_filtered.to_csv(OUTPUT_FILE, sep=\"\\t\", index=False, header=False)\n",
        "\n",
        "# Show result\n",
        "print(f\" Filtered captions: {len(df_filtered)}\")\n",
        "print(\" Saved to:\", OUTPUT_FILE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0xVDgs_VLWy",
        "outputId": "26e58b95-36a2-4796-dfd0-c7dcf2783d3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Captions dictionary created for 53367 images\n"
          ]
        }
      ],
      "source": [
        "# Set new cleaned file path\n",
        "CAPTIONS_FILE = \"/content/drive/MyDrive/data/filtered_captions_cleaned.tsv\"\n",
        "\n",
        "# Load the cleaned TSV\n",
        "df = pd.read_csv(CAPTIONS_FILE, sep=\"\\t\", names=[\"image\", \"caption\"])\n",
        "df = df.dropna()\n",
        "df[\"image\"] = df[\"image\"].astype(str).str.strip()\n",
        "\n",
        "# Build dictionary: image -> [captions]\n",
        "captions_dict = {}\n",
        "for _, row in df.iterrows():\n",
        "    img_name = row[\"image\"]\n",
        "    caption = row[\"caption\"]\n",
        "    if img_name not in captions_dict:\n",
        "        captions_dict[img_name] = []\n",
        "    captions_dict[img_name].append(caption)\n",
        "\n",
        "print(f\" Captions dictionary created for {len(captions_dict)} images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FubjXkqJVOcO",
        "outputId": "478d38ed-e8d0-4989-f0fb-8e80deaae742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Extracting image features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▍      | 18521/53367 [49:37<13:56:42,  1.44s/it]/usr/local/lib/python3.11/dist-packages/PIL/Image.py:3402: DecompressionBombWarning: Image size (93950400 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "100%|██████████| 53367/53367 [3:51:58<00:00,  3.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Saved features for 53367 images to /content/drive/MyDrive/data/image_features.pt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load ResNet50 and remove final classification layer\n",
        "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
        "resnet.eval().cuda()\n",
        "\n",
        "# Image transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Output path\n",
        "FEATURES_PATH = \"/content/drive/MyDrive/data/image_features.pt\"\n",
        "features_dict = {}\n",
        "\n",
        "print(\" Extracting image features...\")\n",
        "with torch.no_grad():\n",
        "    for img_name in tqdm(captions_dict.keys()):\n",
        "        img_path = os.path.join(IMAGE_FOLDER, img_name)\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            img_tensor = transform(image).unsqueeze(0).cuda()\n",
        "            feature = resnet(img_tensor).view(-1).cpu()  # Flatten to 2048\n",
        "            features_dict[img_name] = feature\n",
        "        except Exception as e:\n",
        "            print(f\" Error with {img_name}: {e}\")\n",
        "\n",
        "# Save features\n",
        "torch.save(features_dict, FEATURES_PATH)\n",
        "print(f\" Saved features for {len(features_dict)} images to {FEATURES_PATH}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0zg4nM2xazR0"
      },
      "outputs": [],
      "source": [
        "# Load precomputed features\n",
        "features_dict = torch.load(\"/content/drive/MyDrive/data/image_features.pt\")\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class CaptionFeatureDataset(Dataset):\n",
        "    def __init__(self, features_dict, captions_dict, word2idx, max_len=22):\n",
        "        self.image_names = list(features_dict.keys())\n",
        "        self.features_dict = features_dict\n",
        "        self.captions_dict = captions_dict\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_names[idx]\n",
        "        feature = self.features_dict[img_name]\n",
        "        caption = random.choice(self.captions_dict[img_name]).split()\n",
        "\n",
        "        # Add <SOS> and <EOS>\n",
        "        caption = [\"<SOS>\"] + caption + [\"<EOS>\"]\n",
        "        tokens = [self.word2idx.get(w, self.word2idx[\"<PAD>\"]) for w in caption]\n",
        "        tokens = tokens[:self.max_len]\n",
        "        tokens += [self.word2idx[\"<PAD>\"]] * (self.max_len - len(tokens))\n",
        "\n",
        "        return feature, torch.tensor(tokens, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0srvVNO3dcNr",
        "outputId": "48674cf8-ffdd-41b4-f997-7002836ab2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Vocabulary size: 4903\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Get all captions from the cleaned dict\n",
        "all_captions = sum(captions_dict.values(), [])\n",
        "\n",
        "# Split and flatten all words\n",
        "words = [word for caption in all_captions for word in caption.split()]\n",
        "most_common = Counter(words).most_common(4900)\n",
        "\n",
        "# Special tokens first\n",
        "vocab = [\"<PAD>\", \"<SOS>\", \"<EOS>\"] + [w for w, _ in most_common]\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "VOCAB_SIZE = len(vocab)\n",
        "\n",
        "print(f\" Vocabulary size: {VOCAB_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CZNvYnMtSCvm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "dataset = CaptionFeatureDataset(features_dict, captions_dict, word2idx, max_len=22)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "-kiI5QumdkqF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Transformer_Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_encoding = nn.Parameter(torch.zeros(1, 22, embed_size))  # Max_len=22\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=embed_size,\n",
        "            nhead=8,\n",
        "            dim_feedforward=hidden_size,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions, mask=None):\n",
        "        batch_size = captions.size(0)\n",
        "        seq_len = captions.size(1)\n",
        "\n",
        "        # Embed captions and add positional encoding\n",
        "        embedded = self.embedding(captions) + self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        # Create causal mask for autoregressive decoding\n",
        "        if mask is None:\n",
        "            mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(features.device)\n",
        "\n",
        "        # Features as memory (B, 1, embed_size) for cross-attention\n",
        "        memory = features.unsqueeze(1)  # No need to repeat across time\n",
        "\n",
        "        # Decode\n",
        "        output = self.decoder(tgt=embedded, memory=memory, tgt_mask=mask)\n",
        "        return self.fc(output)\n",
        "\n",
        "# Update initialization\n",
        "decoder = Transformer_Decoder(embed_size=256, vocab_size=VOCAB_SIZE, hidden_size=512, num_layers=3).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "hD_1Z58FeWwb"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "decoder = Transformer_Decoder(embed_size=256, vocab_size=VOCAB_SIZE, hidden_size=512, num_layers=3).to(device)\n",
        "project_features = nn.Linear(2048, 256).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<PAD>\"])\n",
        "optimizer = torch.optim.Adam(list(decoder.parameters()) + list(project_features.parameters()), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MsTvhT5fHxn",
        "outputId": "86a00b40-e855-46b0-cfd8-4ca300e81ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Started...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:25<00:00, 59.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Train Loss: 4.0981, Val Loss: 4.1179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:24<00:00, 60.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Train Loss: 3.8434, Val Loss: 4.0562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:25<00:00, 59.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Train Loss: 3.6636, Val Loss: 4.0197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:24<00:00, 60.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Train Loss: 3.5217, Val Loss: 4.0213\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:25<00:00, 59.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Train Loss: 3.3979, Val Loss: 4.0502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:24<00:00, 60.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 - Train Loss: 3.2905, Val Loss: 4.0473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:25<00:00, 59.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 - Train Loss: 3.1889, Val Loss: 4.0934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:24<00:00, 60.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 - Train Loss: 3.1000, Val Loss: 4.1343\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:24<00:00, 60.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 - Train Loss: 3.0208, Val Loss: 4.1620\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:24<00:00, 60.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 - Train Loss: 2.9478, Val Loss: 4.2022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:25<00:00, 59.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 - Train Loss: 2.8807, Val Loss: 4.2324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:24<00:00, 60.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 - Train Loss: 2.8177, Val Loss: 4.2887\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:25<00:00, 59.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 - Train Loss: 2.7635, Val Loss: 4.3124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:24<00:00, 60.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 - Train Loss: 2.7081, Val Loss: 4.3546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:24<00:00, 60.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 - Train Loss: 2.6588, Val Loss: 4.3732\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:25<00:00, 59.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 - Train Loss: 2.6138, Val Loss: 4.4039\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:25<00:00, 59.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 - Train Loss: 2.5705, Val Loss: 4.4414\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:25<00:00, 59.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 - Train Loss: 2.5318, Val Loss: 4.4865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:25<00:00, 59.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 - Train Loss: 2.4923, Val Loss: 4.5142\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1501/1501 [00:25<00:00, 59.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 - Train Loss: 2.4566, Val Loss: 4.5443\n",
            "Training Finished!\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "\n",
        "NUM_EPOCHS = 20  \n",
        "best_loss = float('inf')\n",
        "\n",
        "print(\"Training Started...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    decoder.train()\n",
        "    project_features.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for features, captions in tqdm(train_loader):  \n",
        "        features, captions = features.to(device), captions.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        projected = project_features(features)  # Shape: [batch_size, 256]\n",
        "        output = decoder(projected, captions[:, :-1])  # Input excludes <EOS>, Shape: [batch_size, seq_len-1, VOCAB_SIZE]\n",
        "\n",
        "        # Compute loss (target excludes <SOS>)\n",
        "        # Use contiguous() to ensure memory layout is compatible with view()\n",
        "        loss = criterion(output.view(-1, VOCAB_SIZE), captions[:, 1:].contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    decoder.eval()\n",
        "    project_features.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for features, captions in val_loader:\n",
        "            features, captions = features.to(device), captions.to(device)\n",
        "            projected = project_features(features)\n",
        "            output = decoder(projected, captions[:, :-1])\n",
        "            loss = criterion(output.view(-1, VOCAB_SIZE), captions[:, 1:].contiguous().view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        torch.save({'decoder': decoder.state_dict(), 'project_features': project_features.state_dict()}, 'best_model.pt')\n",
        "\n",
        "print(\"Training Finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtzE6we1i_MD",
        "outputId": "2b69f34e-90d3-43c6-8ae2-3f544c0b5801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Caption: decorating christmas tree with hugs a christmas tree\n"
          ]
        }
      ],
      "source": [
        "def beam_search_caption(image_path, resnet, transform, decoder, project_features, word2idx, idx2word, beam_width=5, max_len=22):\n",
        "    decoder.eval()\n",
        "    project_features.eval()\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        cnn_feat = resnet(image_tensor).view(1, -1)\n",
        "        img_embed = project_features(cnn_feat)\n",
        "\n",
        "    # Beam search\n",
        "    sequences = [[ [word2idx[\"<SOS>\"]], 0.0 ]]  # (sequence, log_prob)\n",
        "    completed = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        candidates = []\n",
        "        for seq, score in sequences:\n",
        "            if seq[-1] == word2idx[\"<EOS>\"]:\n",
        "                completed.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            input_seq = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = decoder(img_embed, input_seq)\n",
        "                probs = torch.softmax(output[:, -1, :], dim=-1)\n",
        "                topk = torch.topk(probs, beam_width)\n",
        "\n",
        "            for i in range(beam_width):\n",
        "                token = topk.indices[0, i].item()\n",
        "                token_prob = topk.values[0, i].item()\n",
        "                new_seq = seq + [token]\n",
        "                new_score = score + torch.log(torch.tensor(token_prob + 1e-10)).item()\n",
        "                candidates.append((new_seq, new_score))\n",
        "\n",
        "        # Select top beam_width candidates\n",
        "        sequences = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "        if len(sequences) == 0:\n",
        "            break\n",
        "\n",
        "    # Include completed sequences\n",
        "    all_sequences = sequences + completed\n",
        "    best_seq = sorted(all_sequences, key=lambda x: x[1], reverse=True)[0][0]\n",
        "\n",
        "    caption = [idx2word[t] for t in best_seq if t not in [word2idx[\"<PAD>\"], word2idx[\"<SOS>\"], word2idx[\"<EOS>\"]]]\n",
        "    return \" \".join(caption)\n",
        "\n",
        "# Test it \n",
        "test_image_path = \"/content/drive/MyDrive/data/loadedimages/14.jpg\"\n",
        "caption = beam_search_caption(test_image_path, resnet, transform, decoder, project_features, word2idx, idx2word)\n",
        "print(\"Generated Caption:\", caption)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
