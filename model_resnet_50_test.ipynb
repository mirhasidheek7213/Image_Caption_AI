{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWaEFyPjUzWb",
        "outputId": "45e92442-2d92-44de-95a7-5d501c132b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzTleGowaZl6",
        "outputId": "25776a3a-5d5d-4466-f72d-955e43481fb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded features for 53367 images from /content/drive/MyDrive/data/image_features.pt\n"
          ]
        }
      ],
      "source": [
        "# File paths (already saved)\n",
        "FEATURES_PATH = \"/content/drive/MyDrive/data/image_features.pt\"\n",
        "CAPTIONS_FILE = \"/content/drive/MyDrive/data/filtered_captions_cleaned.tsv\"\n",
        "IMAGE_FOLDER = \"/content/drive/MyDrive/data/loadedimages\"  # For inference\n",
        "\n",
        "# Load precomputed image features\n",
        "features_dict = torch.load(FEATURES_PATH)\n",
        "print(f\"Loaded features for {len(features_dict)} images from {FEATURES_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0xVDgs_VLWy",
        "outputId": "3f213006-c9c8-4402-c555-43eb1541465e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Captions dictionary created for 53367 images\n"
          ]
        }
      ],
      "source": [
        "# Load captions and build captions dictionary\n",
        "df = pd.read_csv(CAPTIONS_FILE, sep=\"\\t\", names=[\"image\", \"caption\"])\n",
        "df = df.dropna()\n",
        "df[\"image\"] = df[\"image\"].astype(str).str.strip()\n",
        "captions_dict = {}\n",
        "for _, row in df.iterrows():\n",
        "    img_name = row[\"image\"]\n",
        "    caption = row[\"caption\"]\n",
        "    if img_name not in captions_dict:\n",
        "        captions_dict[img_name] = []\n",
        "    captions_dict[img_name].append(caption)\n",
        "print(f\"Captions dictionary created for {len(captions_dict)} images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FubjXkqJVOcO",
        "outputId": "0b386d85-8633-41ce-a4cf-0e5075c7d50a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 16399\n"
          ]
        }
      ],
      "source": [
        "# Build larger vocabulary\n",
        "all_captions = sum(captions_dict.values(), [])\n",
        "words = [word for caption in all_captions for word in caption.split()]\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Option 1: Take the 10,000 most common words (adjustable)\n",
        "# most_common = word_counts.most_common(10000)\n",
        "# vocab = [\"<PAD>\", \"<SOS>\", \"<EOS>\"] + [w for w, _ in most_common]\n",
        "\n",
        "# Option 2: Use all unique words (default)\n",
        "vocab = [\"<PAD>\", \"<SOS>\", \"<EOS>\"] + list(word_counts.keys())\n",
        "\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "VOCAB_SIZE = len(vocab)\n",
        "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zg4nM2xazR0"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class CaptionFeatureDataset(Dataset):\n",
        "    def __init__(self, features_dict, captions_dict, word2idx, max_len=22):\n",
        "        self.image_names = list(features_dict.keys())\n",
        "        self.features_dict = features_dict\n",
        "        self.captions_dict = captions_dict\n",
        "        self.word2idx = word2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_names[idx]\n",
        "        feature = self.features_dict[img_name]\n",
        "        caption = random.choice(self.captions_dict[img_name]).split()\n",
        "        caption = [\"<SOS>\"] + caption + [\"<EOS>\"]\n",
        "        tokens = [self.word2idx.get(w, self.word2idx[\"<PAD>\"]) for w in caption]\n",
        "        tokens = tokens[:self.max_len] + [self.word2idx[\"<PAD>\"]] * (self.max_len - len(tokens))\n",
        "        return feature, torch.tensor(tokens, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0srvVNO3dcNr",
        "outputId": "4dc53648-f6fa-4710-9790-a204803dae50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set size: 42693, Validation set size: 10674\n"
          ]
        }
      ],
      "source": [
        "# Split dataset into train and validation\n",
        "dataset = CaptionFeatureDataset(features_dict, captions_dict, word2idx, max_len=22)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "print(f\"Training set size: {len(train_dataset)}, Validation set size: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZNvYnMtSCvm"
      },
      "outputs": [],
      "source": [
        "# Transformer Decoder\n",
        "class Transformer_Decoder(nn.Module):\n",
        "    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.pos_encoding = nn.Parameter(torch.zeros(1, 22, embed_size))\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=embed_size, nhead=8, dim_feedforward=hidden_size, dropout=0.1, batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions, mask=None):\n",
        "        batch_size = captions.size(0)\n",
        "        seq_len = captions.size(1)\n",
        "        embedded = self.embedding(captions) + self.pos_encoding[:, :seq_len, :]\n",
        "        if mask is None:\n",
        "            mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(features.device)\n",
        "        memory = features.unsqueeze(1)\n",
        "        output = self.decoder(tgt=embedded, memory=memory, tgt_mask=mask)\n",
        "        return self.fc(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKS09V995bZd"
      },
      "outputs": [],
      "source": [
        "# Training setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "decoder = Transformer_Decoder(embed_size=256, vocab_size=VOCAB_SIZE, hidden_size=512, num_layers=3).to(device)\n",
        "project_features = nn.Linear(2048, 256).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx[\"<PAD>\"])\n",
        "optimizer = torch.optim.Adam(list(decoder.parameters()) + list(project_features.parameters()), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kiI5QumdkqF",
        "outputId": "46a2c0f9-f476-40e7-883c-14c99c0b55fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Started...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 65.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Train Loss: 5.2449, Val Loss: 4.7580\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 66.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Train Loss: 4.4471, Val Loss: 4.5718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 66.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Train Loss: 4.0750, Val Loss: 4.5411\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 66.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Train Loss: 3.7862, Val Loss: 4.5650\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 66.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Train Loss: 3.5416, Val Loss: 4.6219\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:19<00:00, 66.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 - Train Loss: 3.3344, Val Loss: 4.6916\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:19<00:00, 67.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 - Train Loss: 3.1639, Val Loss: 4.7695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:19<00:00, 67.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 - Train Loss: 3.0252, Val Loss: 4.8417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 66.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 - Train Loss: 2.9044, Val Loss: 4.9196\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:19<00:00, 66.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 - Train Loss: 2.7994, Val Loss: 4.9918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 66.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 - Train Loss: 2.7050, Val Loss: 5.0596\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 66.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 - Train Loss: 2.6241, Val Loss: 5.1231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 66.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 - Train Loss: 2.5503, Val Loss: 5.1910\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 66.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 - Train Loss: 2.4773, Val Loss: 5.2793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:19<00:00, 66.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 - Train Loss: 2.4177, Val Loss: 5.3119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:19<00:00, 66.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 - Train Loss: 2.3584, Val Loss: 5.3496\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 66.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 - Train Loss: 2.3065, Val Loss: 5.4024\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:19<00:00, 67.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 - Train Loss: 2.2538, Val Loss: 5.4898\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 66.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 - Train Loss: 2.2068, Val Loss: 5.5250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1335/1335 [00:20<00:00, 65.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 - Train Loss: 2.1633, Val Loss: 5.5557\n",
            "Training Finished!\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "NUM_EPOCHS = 20\n",
        "best_loss = float('inf')\n",
        "print(\"Training Started...\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    decoder.train()\n",
        "    project_features.train()\n",
        "    total_train_loss = 0\n",
        "    for features, captions in tqdm(train_loader):\n",
        "        features, captions = features.to(device), captions.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        projected = project_features(features)\n",
        "        output = decoder(projected, captions[:, :-1])\n",
        "        loss = criterion(output.view(-1, VOCAB_SIZE), captions[:, 1:].contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    decoder.eval()\n",
        "    project_features.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for features, captions in val_loader:\n",
        "            features, captions = features.to(device), captions.to(device)\n",
        "            projected = project_features(features)\n",
        "            output = decoder(projected, captions[:, :-1])\n",
        "            loss = criterion(output.view(-1, VOCAB_SIZE), captions[:, 1:].contiguous().view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        torch.save({'decoder': decoder.state_dict(), 'project_features': project_features.state_dict()}, '/content/drive/MyDrive/data/best_model.pt')\n",
        "\n",
        "print(\"Training Finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-XUJmGO1RF4"
      },
      "outputs": [],
      "source": [
        "# Load ResNet50 for inference\n",
        "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
        "resnet.eval().cuda()\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD_1Z58FeWwb"
      },
      "outputs": [],
      "source": [
        "# Beam search for caption generation\n",
        "def beam_search_caption(image_path, resnet, transform, decoder, project_features, word2idx, idx2word, beam_width=5, max_len=22):\n",
        "    decoder.eval()\n",
        "    project_features.eval()\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_tensor = transform(image).unsqueeze(0).cuda()\n",
        "    with torch.no_grad():\n",
        "        cnn_feat = resnet(image_tensor).view(1, -1)\n",
        "        img_embed = project_features(cnn_feat)\n",
        "    sequences = [[ [word2idx[\"<SOS>\"]], 0.0 ]]\n",
        "    completed = []\n",
        "    for _ in range(max_len):\n",
        "        candidates = []\n",
        "        for seq, score in sequences:\n",
        "            if seq[-1] == word2idx[\"<EOS>\"]:\n",
        "                completed.append((seq, score))\n",
        "                continue\n",
        "            input_seq = torch.tensor([seq], dtype=torch.long).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = decoder(img_embed, input_seq)\n",
        "                probs = torch.softmax(output[:, -1, :], dim=-1)\n",
        "                topk = torch.topk(probs, beam_width)\n",
        "            for i in range(beam_width):\n",
        "                token = topk.indices[0, i].item()\n",
        "                token_prob = topk.values[0, i].item()\n",
        "                new_seq = seq + [token]\n",
        "                new_score = score + torch.log(torch.tensor(token_prob + 1e-10)).item()\n",
        "                candidates.append((new_seq, new_score))\n",
        "        sequences = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "        if len(sequences) == 0:\n",
        "            break\n",
        "    all_sequences = sequences + completed\n",
        "    best_seq = sorted(all_sequences, key=lambda x: x[1], reverse=True)[0][0]\n",
        "    caption = [idx2word[t] for t in best_seq if t not in [word2idx[\"<PAD>\"], word2idx[\"<SOS>\"], word2idx[\"<EOS>\"]]]\n",
        "    return \" \".join(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MsTvhT5fHxn",
        "outputId": "31bdebae-184b-423c-b270-8c1a4496e381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Caption: i love the smell of the flowers !\n"
          ]
        }
      ],
      "source": [
        "# Test inference\n",
        "test_image_path = \"/content/drive/MyDrive/data/loadedimages/106.jpg\"\n",
        "caption = beam_search_caption(test_image_path, resnet, transform, decoder, project_features, word2idx, idx2word)\n",
        "print(\"Generated Caption:\", caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dJCCJUK1AzE"
      },
      "outputs": [],
      "source": [
        "# Load saved model\n",
        "checkpoint = torch.load('/content/drive/MyDrive/data/best_model.pt')\n",
        "decoder.load_state_dict(checkpoint['decoder'])\n",
        "project_features.load_state_dict(checkpoint['project_features'])\n",
        "decoder.eval()\n",
        "project_features.eval()\n",
        "print(\"Loaded saved model from /content/drive/MyDrive/data/best_model.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
